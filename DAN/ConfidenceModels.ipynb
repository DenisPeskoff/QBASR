{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from tqdm import tqdm_notebook\n",
    "import numpy as np\n",
    "import string\n",
    "import time\n",
    "from typing import List, Optional, Dict\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from tqdm import tqdm_notebook \n",
    "from collections import defaultdict\n",
    "\n",
    "#torch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.autograd import Variable\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "#torchtext.  Dataset is a custom file for handling QuizBowl data\n",
    "#from dataset import QuizBowl\n",
    "from torchtext.data.field import Field\n",
    "from torchtext.data.iterator import Iterator\n",
    "\n",
    "import dataset_confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN, self).__init__()\n",
    "        \n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        #It takes batch norm and dropout as well.\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "\n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        #if the text exists, run it through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnormed_dropped)\n",
    "            return self.classifier(nonlinear )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConfidenceLearner(nn.Module):\n",
    "    def __init__(self, embeddings_dim, confidences_dim):\n",
    "        super(ConfidenceLearner, self).__init__()\n",
    "        self.transform = nn.Linear((embeddings_dim + confidences_dim), 1)\n",
    "    \n",
    "    def forward(self, embeds, confs):\n",
    "        concat = torch.cat((embeds, confs), -1)\n",
    "        data = torch.sigmoid(self.transform(concat))\n",
    "        return data\n",
    "    \n",
    "    \n",
    "#train with SimpleConfidenceLearner.  Then freeze embeddings and train ConfidenceLearner.\n",
    "#loop through each parameter and set weights equal to it\n",
    "\n",
    "class SimpleConfidenceLearner(nn.Module):\n",
    "    def __init__(self, embeddings_dim, confidences_dim):\n",
    "        #does nothing\n",
    "        super(SimpleConfidenceLearner, self).__init__()\n",
    "        pass\n",
    "    \n",
    "    def forward(self, embeds, confs):\n",
    "        return confs.unsqueeze(2)\n",
    "    \n",
    "class DAN_Confidences(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_Confidences, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #confidences are learned from word_embeddings and respective word_confidence\n",
    "        self.confidences = SimpleConfidenceLearner(embedding_dim, 1)\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),  #make this h1_dim+1 when appeneded\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            confidences = Variable(confidences).cuda()\n",
    "            confidences = self.confidences(embed, confidences)\n",
    "            multiplied = embed * confidences\n",
    "            averaged = self._pool(multiplied, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnormed_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnormed_dropped)\n",
    "            return self.classifier(nonlinear )\n",
    "        \n",
    "        #layer.weights.data[:, -1] = pretrainedweights\n",
    "        #layer.bias.data[: -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN_Confidences_Softmax(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_Confidences_Softmax, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim+1, answer_size),  #make this h1_dim+1 when appeneded\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self, embed, lengths, batch_size):\n",
    "        return embed.sum(1) / lengths.view(batch_size, -1)\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "        \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            #do this elsewhere\n",
    "            confidences = Variable(confidences).cuda()                   \n",
    "            averaged = self._pool(embed, lengths['text'].float(), text_input.size()[0])\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)  \n",
    "            batchnorm_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnorm_dropped)\n",
    "            expanded = torch.cat((nonlinear,confidences.mean(dim=1).unsqueeze(-1)), 1)\n",
    "            return self.classifier(expanded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAN_WeightedMean(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(DAN_WeightedMean, self).__init__()\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #self.text_embeddings.weight.requires_grad = False\n",
    "        #freeze the embeddings\n",
    "        \n",
    "        #layers used in the network\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.small_dropout = nn.Dropout(p=0.15)\n",
    "        \n",
    "        self.nonlinear = nn.Sigmoid()\n",
    "        self.hidden = nn.Linear(embedding_dim, h1_dim)\n",
    "        self.batch_norm = nn.BatchNorm1d(h1_dim)\n",
    "        \n",
    "        #self.confidences = ConfidenceLearner(embedding_dim, 1) # ADDED IN FOR VARIATION\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15)\n",
    "        )\n",
    "        \n",
    "    def _pool(self,  embed, lengths, confidences):\n",
    "        embed = embed * confidences.unsqueeze(2).expand_as(embed) \n",
    "        embed = embed.sum(1)\n",
    "        return embed / confidences.sum(dim = 1).unsqueeze(1).expand_as(embed)\n",
    "                    \n",
    "        #for learning variation\n",
    "              #pass in output of CONFIDENCE LEARNER into POOOl\n",
    "              #dimensions will be the same, just learning new value for confidence\n",
    "    \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        for key in lengths:\n",
    "            if not isinstance(lengths[key], Variable):\n",
    "                lengths[key] = Variable(lengths[key].float(), volatile=not self.training)\n",
    "                \n",
    "        if self.text_field is not None:\n",
    "            text_input = input_['text']\n",
    "            embed = self.text_embeddings(text_input)\n",
    "            confidences = Variable(confidences).cuda() \n",
    "            #confidences = self.confidences(embed, confidences).squeeze() #ADDED IN FOR VARIATION\n",
    "            averaged = self._pool(embed, lengths['text'].float(), confidences)\n",
    "            averaged_dropped = self.small_dropout(averaged)\n",
    "            hidden_layer = self.hidden(averaged_dropped)\n",
    "            batchnorm_dropped = self.large_dropout(self.batch_norm(hidden_layer))\n",
    "            nonlinear = self.nonlinear(batchnorm_dropped)\n",
    "            return self.classifier(batchnorm_dropped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(RNN, self).__init__()\n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        #self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #initiate hidden dimensions and allow them to be referenced by init_hidden\n",
    "        self.hidden_dim = h1_dim\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.rnn = nn.GRU(embedding_dim, h1_dim)\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.batch = nn.BatchNorm1d(h1_dim)\n",
    "        self.nonlinearity = nn.Sigmoid()\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15))\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size = 32):\n",
    "        \"\"\"\n",
    "        Return variables that we can use as h_0 and c_0. \n",
    "        \"\"\"\n",
    "        return (Variable(torch.zeros(1, batch_size, self.hidden_dim ).cuda()),\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda()))\n",
    " \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums): \n",
    "        #Run text through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        text_input = input_['text']\n",
    "            \n",
    "        embed = self.text_embeddings(text_input)\n",
    "        lengths_cpu = lengths['text'].cpu()\n",
    "        # pack the batch\n",
    "        packed = pack_padded_sequence(embed, list(lengths_cpu),\n",
    "                                      batch_first=True)\n",
    "        out_packed, self.hidden = self.rnn(packed)     \n",
    "        x = self.large_dropout(self.batch(self.hidden[0].squeeze()))\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Softmax(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(RNN_Softmax, self).__init__()\n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #freeze the embeddings\n",
    "        #self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #initiate hidden dimensions and allow them to be referenced by init_hidden\n",
    "        self.hidden_dim = h1_dim\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.rnn = nn.GRU(embedding_dim, h1_dim)\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.batch = nn.BatchNorm1d(h1_dim)\n",
    "        self.nonlinearity = nn.Sigmoid()\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim+1, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15))\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size = 32):\n",
    "        \"\"\"\n",
    "        Return variables that we can use as h_0 and c_0. \n",
    "        \"\"\"\n",
    "        return (Variable(torch.zeros(1,batch_size, self.hidden_dim).cuda()),\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda()))\n",
    " \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "        \n",
    "        confidences = Variable(confidences).cuda()                   \n",
    "        #Run text through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        text_input = input_['text']\n",
    "\n",
    "        embed = self.text_embeddings(text_input)\n",
    "        lengths_cpu = lengths['text'].cpu()\n",
    "         # pack the batch\n",
    "        packed = pack_padded_sequence(embed, list(lengths_cpu),\n",
    "                                      batch_first=True)\n",
    "        out_packed, self.hidden = self.rnn(packed)\n",
    "         \n",
    "        x = self.large_dropout(self.batch(self.hidden[0].squeeze()))\n",
    "        expanded = torch.cat((x,confidences.mean(dim=1).unsqueeze(-1)), 1)\n",
    "        x = self.classifier(expanded)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN_Confidences(nn.Module):\n",
    "    def __init__(self, embedding_dim, \n",
    "                 h1_dim, text_field, answer_size):\n",
    "        super(RNN_Confidences, self).__init__()\n",
    "        #initialize the vocab from the text field (passed in from train_iter) and pad\n",
    "        text_vocab = text_field.vocab\n",
    "        self.text_vocab_size = len(text_vocab)\n",
    "        text_pad_idx = text_vocab.stoi[text_field.pad_token]\n",
    "        \n",
    "        #run the vocab through Glove Embeddings\n",
    "        self.text_embeddings = nn.Embedding(self.text_vocab_size, embedding_dim, padding_idx=text_pad_idx)\n",
    "        self.text_field = text_field\n",
    "        \n",
    "        #set the unknown items to the mean embedding and make them cuda()\n",
    "        mean_emb = text_vocab.vectors.mean(0)\n",
    "        text_vocab.vectors[text_vocab.stoi[text_field.unk_token]] = mean_emb\n",
    "        self.text_embeddings.weight.data = text_vocab.vectors.cuda()\n",
    "        \n",
    "        #confidences are learned from word_embeddings and respective word_confidence\n",
    "        self.confidences = SimpleConfidenceLearner(embedding_dim, 1)\n",
    "    \n",
    "        #freeze the embeddings\n",
    "        #self.text_embeddings.weight.requires_grad = False \n",
    "        \n",
    "        #initiate hidden dimensions and allow them to be referenced by init_hidden\n",
    "        self.hidden_dim = h1_dim\n",
    "        self.hidden = self.init_hidden()\n",
    "        self.rnn = nn.LSTM(embedding_dim, h1_dim)\n",
    "        self.large_dropout = nn.Dropout(p=0.265)\n",
    "        self.batch = nn.BatchNorm1d(h1_dim)\n",
    "        self.nonlinearity = nn.Sigmoid()\n",
    "        \n",
    "        #the classifier converts the hidden dimensions into the answers.  \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(h1_dim, answer_size),\n",
    "            nn.BatchNorm1d(answer_size),\n",
    "            nn.Dropout(.15))\n",
    "        \n",
    "    \n",
    "    def init_hidden(self, batch_size = 32):\n",
    "        \"\"\"\n",
    "        Return variables that we can use as h_0 and c_0. \n",
    "        \"\"\"\n",
    "        return (Variable(torch.zeros(1,batch_size, self.hidden_dim).cuda()),\n",
    "                Variable(torch.zeros(1, batch_size, self.hidden_dim).cuda()))\n",
    " \n",
    "    def forward(self, input_: Dict[str, Variable], lengths: Dict, qnums, confidences): \n",
    "\n",
    "        #Run text through embeddings, pool, dropout, and then run it through a hidden layer\n",
    "        text_input = input_['text']\n",
    "                 \n",
    "        embed = self.text_embeddings(text_input)\n",
    "        \n",
    "        confidences = Variable(confidences).cuda()\n",
    "        confidences = self.confidences(embed, confidences)\n",
    "        multiplied = embed * confidences\n",
    "        \n",
    "        lengths_cpu = lengths['text'].cpu()\n",
    "        # pack the batch\n",
    "        packed = pack_padded_sequence(multiplied, list(lengths_cpu),\n",
    "                                      batch_first=True)\n",
    "        out_packed, self.hidden = self.rnn(packed)\n",
    "        x = self.large_dropout(self.batch(self.hidden[0].squeeze()))\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "importlib.reload(dataset_confidence)\n",
    "train_iter_asr, val_iter_asr, dev_iter_asr = dataset_confidence.QuizBowl.iters(\n",
    "            batch_size=512,\n",
    "            lower= True,\n",
    "            use_wiki=False,  #irrelevant\n",
    "            n_wiki_sentences=5, #irrelevant \n",
    "            replace_title_mentions='',\n",
    "            combined_ngrams=True,\n",
    "            unigrams=True, \n",
    "            bigrams=False, #irrelevant \n",
    "            trigrams=False, #irrelevant \n",
    "            combined_max_vocab_size=300000,\n",
    "            unigram_max_vocab_size= None, \n",
    "            bigram_max_vocab_size=50000, #irrelevant \n",
    "            trigram_max_vocab_size=50000 #irrelevant \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(iterator: Iterator, CONFIDENCE = False):\n",
    "        is_train = iterator.train\n",
    "        batch_accuracies = []\n",
    "        batch_losses = []\n",
    "        epoch_start = time.time()\n",
    "        for batch in iterator:\n",
    "            input_dict = {}\n",
    "            lengths_dict = {}\n",
    "            if hasattr(batch, 'text'):\n",
    "                text, lengths = batch.text             \n",
    "                input_dict['text'] = text\n",
    "                lengths_dict['text'] = lengths\n",
    "\n",
    "            page = batch.page    \n",
    "            qnums = batch.qnum.cuda()\n",
    "\n",
    "            if is_train:\n",
    "                model.zero_grad()\n",
    "            \n",
    "            if CONFIDENCE:\n",
    "                confidences = batch.confidence\n",
    "                out = model(input_dict, lengths_dict, qnums, confidences)\n",
    "            else:\n",
    "                out = model(input_dict, lengths_dict, qnums)\n",
    "                \n",
    "            _, preds = torch.max(out, 1)\n",
    "      \n",
    "            accuracy = torch.mean(torch.eq(preds, page).float()).data[0]           \n",
    "            batch_loss = loss_function(out, page)\n",
    "            if is_train:\n",
    "                batch_loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm(model.parameters(), .25)\n",
    "                optimizer.step()\n",
    "\n",
    "            batch_accuracies.append(accuracy)\n",
    "            batch_losses.append(batch_loss.data[0])\n",
    "\n",
    "        epoch_end = time.time()\n",
    "\n",
    "        return np.mean(batch_accuracies), np.mean(batch_losses), epoch_end - epoch_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set the dimensions and epochs for model\n",
    "EMBEDDING_DIM = 300\n",
    "HIDDEN_DIM = 1000\n",
    "EPOCH = 50\n",
    "\n",
    "#extract fields to determine vocabulary size of answers\n",
    "#UPDATE THIS BETWEEN CLEAN AND ASR\n",
    "fields: Dict[str, Field] = train_iter_asr.dataset.fields\n",
    "page_field = fields['page']\n",
    "ANSWER_SIZE = len(page_field.vocab.stoi)\n",
    " \n",
    "model = DAN(EMBEDDING_DIM,\n",
    "             HIDDEN_DIM,\n",
    "             fields['text'], \n",
    "             ANSWER_SIZE)\n",
    "model = model.cuda()\n",
    "\n",
    "loss_function = nn.CrossEntropyLoss()\n",
    "\n",
    "for i in tqdm_notebook(range(EPOCH)):  \n",
    "    scheduler.step()\n",
    "    #train\n",
    "    model.train()\n",
    "    train_acc, train_loss, train_time = run_epoch(train_iter_asr)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "\n",
    "    #validate\n",
    "    model.eval()\n",
    "    val_acc, val_loss, val_time = run_epoch(val_iter_asr)    \n",
    "    val_losses.append(val_loss)\n",
    "    val_accuracies.append(val_acc)\n",
    "    print (val_acc, val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Model Accuracies\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.plot(val_accuracies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_acc, test_loss, test_time= run_epoch(dev_iter_asr)\n",
    "test_acc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
