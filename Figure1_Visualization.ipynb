{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from typing import Dict\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "from ggplot import *\n",
    "\n",
    "import nltk \n",
    "nltk.download('stopwords') #<-  Likely need this\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "278 225\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['ain', 'until', 'from', \"hasn't\", 'with']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stopwords currently used from nltk, but custom list would be better (e.g. top 1000 ASR'd words)\n",
    "#these should hopefully be the same as before\n",
    "\n",
    "#these are calculated from the data\n",
    "additional_stop = [x[0] for x in [('unk', 74533), ('NULL', 35761), ('the', 3587), ('and', 2949), ('a', 2806), ('to', 2685), ('in', 2103), ('you', 1960), ('or', 1860), ('one', 1712), ('it', 1587), ('on', 1477), \n",
    "                                  ('for', 1394), ('that', 1317), ('three', 1312), ('like', 1165), ('oh', 1163), \n",
    "                                  ('hundred', 1096), ('of', 1087), ('yeah', 1057), ('uh', 970), ('they', 962),\n",
    "                                  ('i', 959), ('know', 945), ('right', 938), ('five', 895), ('so', 783), ('here', 766),\n",
    "                                  ('are', 698), ('is', 690), ('two', 686), ('well', 680), ('h', 679), ('be', 650), \n",
    "                                  ('our', 638), ('hi', 609), ('them', 602), ('out', 601), ('he', 594), ('p', 564), \n",
    "                                  ('its', 548), ('do', 544), ('then', 543), ('now', 510), ('l', 510), ('e', 503), \n",
    "                                  ('no', 496), ('with', 495), ('me', 480), ('who', 476), ('my', 469), ('there', 462), \n",
    "                                  ('seven', 451), ('six', 448), ('man', 440), ('seventy', 436), ('thousand', 424), \n",
    "                                  ('fifty', 424), ('r', 416), ('sixty', 407), ('had', 405), ('which', 403), ('noise', 402),\n",
    "                                  ('nineteen', 400), ('nine', 398), ('eight', 397), ('at', 397), ('thirty', 396),\n",
    "                                  ('this', 396), ('up', 396), ('w', 396), ('over', 390), ('by', 388), ('um', 384), \n",
    "                                  ('not', 381), ('non', 375), ('too', 370), ('four', 369), ('an', 366), ('their', 365), \n",
    "                                  ('as', 361), ('men', 360), ('she', 356), ('twenty', 352), ('theyre', 350), \n",
    "                                  ('him', 345), ('his', 345), ('time', 344), ('have', 340), ('did', 334), \n",
    "                                  ('quote', 329), ('n', 327), ('first', 325), ('after', 324), ('point', 324), \n",
    "                                  ('forty', 324), ('thats', 322), ('from', 321), ('down', 317)]]\n",
    "stop_words.extend(additional_stop)\n",
    "print(len(stop_words), len(set(stop_words)))\n",
    "stop_words = list(set(stop_words))\n",
    "stop_words[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "968135it [00:02, 403017.47it/s]\n"
     ]
    }
   ],
   "source": [
    "#function to calculate how often a term maps to junk.  Normalized by # of junk words, as each one adds up to 100%\n",
    "def get_junk_translations(term: str, tpm: Dict[str, Dict[str, float]]) -> float:\n",
    "    stopwords = stop_words\n",
    "    total = 0.0\n",
    "    count = 0\n",
    "    for alignment in stopwords:\n",
    "        if alignment in tpm[term]:\n",
    "          #  count += 1\n",
    "            total += tpm[term][alignment]\n",
    "   #if count != 0:    \n",
    "        #total = total/count\n",
    "    return total\n",
    "\n",
    "#read the translation matrix.  Precomputed with Gyza.\n",
    "def read_translation_probability_matrix(fp) -> Dict[str, Dict[str, float]]:\n",
    "    probabilities = defaultdict(lambda: defaultdict(float))\n",
    "    for line in tqdm(fp):\n",
    "        w1, w0, prob = line.strip().split()\n",
    "        probabilities[w0][w1] = float(prob)\n",
    "    return probabilities\n",
    "\n",
    "#lex.f2e is the opposite mapping\n",
    "lex_file = 'lex.e2f'\n",
    "\n",
    "#these are clean sentences\n",
    "with open('sent.cln') as fp:\n",
    "    data = [l for l in fp]\n",
    "    \n",
    "tpm = read_translation_probability_matrix(open(lex_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 193910/193910 [00:03<00:00, 54289.90it/s]\n"
     ]
    }
   ],
   "source": [
    "c = CountVectorizer()\n",
    "\n",
    "# fit_transform returns a scipy sparse matrix of dimensions:\n",
    "#  (number_of_documents, vocab_size)\n",
    "fitted = c.fit_transform(data)\n",
    "term_frequencies = np.array(fitted.sum(0)[0])\n",
    "\n",
    "vocab = ['']*len(c.vocabulary_.keys())\n",
    "for word, ix in c.vocabulary_.items():\n",
    "    vocab[ix] = word\n",
    "\n",
    "array_of_junk_translations = np.array([\n",
    "    get_junk_translations(term, tpm) for term in tqdm(vocab)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_jitter(arr):\n",
    "    stdev = .01*(max(arr)-min(arr))\n",
    "    return arr + np.random.randn(len(arr)) * stdev\n",
    "\n",
    "def jitter(x, y, s=20, c='b', marker='o', cmap=None, norm=None, vmin=None, vmax=None, alpha=None, linewidths=None, verts=None, hold=None, **kwargs):\n",
    "    return plt.scatter(rand_jitter(x), rand_jitter(y), s=s, c=c, marker=marker, cmap=cmap, norm=norm, vmin=vmin, vmax=vmax, alpha=alpha, linewidths=linewidths, verts=verts, hold=hold, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['disinherits', 'disintegrate', 'disintegrated', 'disintegrates', 'disintegrating', 'disintegration', 'disinter', 'disinterest', 'disinterested', 'disinterestedly', 'disinterestedness', 'disinterred', 'disinters', 'disir', 'disjecta', 'disjoin', 'disjoint', 'disjointed', 'disjunction', 'disjunctions']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.3333334, 0.2      , 0.       , 0.       , 0.       , 0.0857143,\n",
       "       1.       , 0.3333333, 0.       , 0.       , 1.       , 1.       ,\n",
       "       0.       , 0.5      , 1.       , 0.       , 0.2083334, 0.       ,\n",
       "       0.       , 0.       ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sanity check\n",
    "print(vocab[50005:50025])\n",
    "array_of_junk_translations[50005:50025]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Issues with 2815 vocab\n",
      "Removed 18427 self-matching words\n",
      "Removed 94845 words that occur once\n"
     ]
    }
   ],
   "source": [
    "#filter data to that which we are interested in\n",
    "count, correct_count, one_count = 0, 0, 0\n",
    "select_term_frequencies = []\n",
    "select_array_of_junk_translations = []\n",
    "select_vocab = []\n",
    "\n",
    "for ix, word in enumerate(vocab):\n",
    "    if tpm[word]:\n",
    "        if max(tpm[word], key=tpm[word].get) == word:\n",
    "            correct_count +=1\n",
    "            pass\n",
    "            #print(tpm[word], word)\n",
    "        else:\n",
    "            if term_frequencies[0][ix] > 1:\n",
    "                select_vocab.append(word)\n",
    "                select_term_frequencies.append(term_frequencies[0][ix])\n",
    "                select_array_of_junk_translations.append(array_of_junk_translations[ix])\n",
    "            else:\n",
    "                one_count+=1\n",
    "            #print(tpm[word], word)\n",
    "    else:\n",
    "        count +=1\n",
    "print(f\"Issues with {count} vocab\")\n",
    "print(f\"Removed {correct_count} self-matching words\")   \n",
    "print(f\"Removed {one_count} words that occur once\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [ 0, .25, .50, .75, 1.001]\n",
    "df = pd.DataFrame({'Word_Log_Term_Frequency': np.log(select_term_frequencies), 'Probability_of_Being_Mapped_to_Junk':select_array_of_junk_translations, 'txt':[(txt +'\\n'+ max(tpm[txt], key=tpm[txt].get)) for txt in select_vocab]})\n",
    "df['binned'] = pd.cut(df['Probability_of_Being_Mapped_to_Junk'], bins,  right = True, include_lowest=True)\n",
    "df['binned_x'] = pd.cut(df['Word_Log_Term_Frequency'], [0, 1, 3, 5, 12], include_lowest=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(df):\n",
    "    sample = df.groupby('binned_x', as_index= False).apply(lambda df: df.sample(100))\n",
    "    sample_dict = {}\n",
    "    count = 0\n",
    "    for ix, item in sample.iterrows():\n",
    "        #print(item)\n",
    "        if (item[4] in sample_dict.keys()):\n",
    "            if item[3] in sample_dict[item[4]]:\n",
    "                pass\n",
    "            else: \n",
    "                sample_dict[item[4]].update({item[3]:count})\n",
    "        else:\n",
    "            sample_dict[item[4]] = {item[3]:count}\n",
    "        count+=1\n",
    "\n",
    "    candidates = []\n",
    "    for keys in sample_dict.keys():\n",
    "        for a in  sample_dict[keys].keys():\n",
    "            candidates.append(sample_dict[keys][a])\n",
    "\n",
    "    labels = sample.iloc[candidates]\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-57-52b8ede72e2b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mggplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binned_x'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'binned'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m#+ theme_light()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgeom_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcolor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"#37BEC0\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m.1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mposition\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'jitter'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# + labs(x = 'log(Term Frequency), by Word', y = 'Probability of Being Junk', title = \"Scoping Information Loss\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mp\u001b[0m \u001b[0;31m#+ geom_label(labels) #+ geom_text(aes(check_overlap=True), sample, size = 10) +\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "df = df.sample(10000)\n",
    "#labels = get_labels(df)\n",
    "#sample = df.sample(10, axis = 1)\n",
    "\n",
    "\n",
    "p = ggplot(aes(x='binned_x', y='binned', label = 'txt'), df)  #+ theme_light() \n",
    "p = p + geom_point(aes(),color = \"#37BEC0\", size = .1, position = 'jitter') # + labs(x = 'log(Term Frequency), by Word', y = 'Probability of Being Junk', title = \"Scoping Information Loss\")\n",
    "p #+ geom_label(labels) #+ geom_text(aes(check_overlap=True), sample, size = 10) +"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:asr]",
   "language": "python",
   "name": "conda-env-asr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
